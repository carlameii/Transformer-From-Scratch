{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer From Scratch\n",
    "\n",
    "### Authors:\n",
    " - Carla Ellefsen\n",
    " - Brendan McKinley\n",
    " - Diya Vinod\n",
    " - Bingshen Lu\n",
    " - Michael Ivanitskiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # default test values -- too small for a real language model, but big enough for testing\n",
    "    d_vocab: int = 10_000  # size of the vocabulary\n",
    "    d_model: int = 128  # dimension of the model\n",
    "    d_mlp: int = 512  # dimension of the MLP (Feed-Forward) layer\n",
    "    n_heads: int = 4  # number of attention heads\n",
    "    d_head: int = 32  # dimension of each attention head\n",
    "    n_layers: int = 6  # number of layers in the transformer\n",
    "    act_fn: type[nn.Module] = nn.ReLU  # activation function\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> tuple[int]:\n",
    "        \"an estimate of the number of parameters\"\n",
    "        return (\n",
    "            self.d_vocab * self.d_model  # embeddings (and tied unembeddings)\n",
    "            + (\n",
    "                self.d_model * self.d_mlp * 2  # mlp weights\n",
    "                + self.d_model + self.d_mlp  # mlp bias\n",
    "                + self.n_heads * (  # number of heads\n",
    "                    4 * self.d_model * self.d_head  # 4 because Q, K, O, V\n",
    "                )\n",
    "            ) * self.n_layers,  # for each layer\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head class implements a single attention head for the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAttentionHead\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: GPTConfig):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention Head Constructor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        print(\"Attention Head Constructor...\")\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.d_vocab = cfg.d_vocab\n",
    "        self.d_model = cfg.d_model\n",
    "        self.d_head = cfg.d_head\n",
    "        self.wq = nn.Linear(self.d_model, self.d_head)  # Linear layer for query\n",
    "        self.wk = nn.Linear(self.d_model, self.d_head)  # Linear layer for key\n",
    "        self.wv = nn.Linear(self.d_model, self.d_head)  # Linear layer for value\n",
    "        self.wo = nn.Linear(self.d_head, self.d_model)  # Linear layer for output\n",
    "\n",
    "    def forward(self,\n",
    "                x: Int[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        def masking_matrix(n_context):\n",
    "            # Create a masking matrix to prevent attending to future tokens\n",
    "            mask = torch.zeros((n_context, n_context))  # Start with all 0s\n",
    "            mask[torch.triu(torch.ones((n_context, n_context)), diagonal=1) == 1] = -float('inf')  # Set above diagonal to -inf\n",
    "            return mask\n",
    "\n",
    "        M = masking_matrix(x.shape[0])  # Generate the masking matrix\n",
    "        wk_out = self.wk(x).transpose(-2, -1)  # Apply key linear layer and transpose\n",
    "        wq_out = self.wq(x)  # Apply query linear layer\n",
    "        softmax_out = F.softmax((wq_out @ wk_out + M), dim=-1)  # Apply softmax to scaled dot-product of queries and keys with masking\n",
    "\n",
    "        wv_out = self.wv(x)  # Apply value linear layer\n",
    "        wo_out = self.wo(wv_out)  # Apply output linear layer\n",
    "\n",
    "        result = softmax_out @ wo_out  # Compute the final output\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMultiHeadedAttention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: GPTConfig):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiHeadedAttention Constructor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        print(\"MultiHeadedAttention Constructor...\")\n",
    "        super().__init__()\n",
    "        self.n_heads = cfg.n_heads\n",
    "        self.d_model = cfg.d_model\n",
    "        self.d_head = cfg.d_head\n",
    "\n",
    "        # Create a list of attention heads\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(cfg) for _ in range(self.n_heads)])\n",
    "\n",
    "        # Linear layer to fix the output size\n",
    "        self.wo = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply each attention head to the input\n",
    "        head_outputs = [head(x) for head in self.attention_heads]\n",
    "        \n",
    "        # Sum the outputs of all attention heads\n",
    "        summed_heads = torch.sum(torch.stack(head_outputs), dim=0)\n",
    "        \n",
    "        # Apply the linear layer to the summed outputs\n",
    "        output = self.wo(summed_heads)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a feed-forward neural network to the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        print(\"MLP Constructor...\")\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = cfg.d_model\n",
    "        self.d_mlp = cfg.d_mlp\n",
    "\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.d_model, self.d_mlp)\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.d_mlp, self.d_model)\n",
    "\n",
    "    def forward(self,\n",
    "                x: Int[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        # Apply first linear layer\n",
    "        out = self.lin1(x)  \n",
    "        # Apply ReLU activation\n",
    "        out = self.relu(out)\n",
    "        # Apply second linear layer\n",
    "        out = self.lin2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a single transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        print(\"TransformerBlock Constructor...\")\n",
    "        super().__init__()\n",
    "        self.multiheadattn = MultiHeadedAttention(cfg)   # Initialize multi-head attention\n",
    "        self.mlp = MLP(cfg)        # Initialize MLP\n",
    "        self.norm1 = nn.RMSNorm(cfg.d_model)  # Layer normalization for attention output\n",
    "        self.norm2 = nn.RMSNorm(cfg.d_model)  # Layer normalization for MLP output\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        attn_output = self.multiheadattn(self.norm1(x))     # Apply layer normalization and multi-head attention\n",
    "        x = x + attn_output         # Add residual connection\n",
    "        mlp_output = self.mlp(self.norm2(x))    # Apply layer normalization and MLP\n",
    "        x = x + mlp_output        # Add residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        print(\"**\"*30)\n",
    "        print(\"Transformer Constructor...\")\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)  # Embedding layer to convert token indices to embeddings\n",
    "        self.unembedding = nn.Linear(cfg.d_model, cfg.d_vocab)  # Linear layer to convert embeddings back to token logits\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])  # List of transformer blocks\n",
    "\n",
    "    def forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "        out = self.embedding(x)  # Apply embedding layer\n",
    "        for block in self.transformer_blocks:\n",
    "            out = block(out)  # Pass through each transformer block\n",
    "        out = F.softmax(self.unembedding(out), dim=-1)  # Apply unembedding layer and softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text data by converting it into tensor representations and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFinder:\n",
    "    def __init__(self, text):\n",
    "        print(\"==\"*30)\n",
    "        print(\"TextFinder Constructor...\")\n",
    "        self.text = text\n",
    "        self.word_index = self.create_word_index(text)  # Create a word index for the given text\n",
    "\n",
    "    def create_word_index(self, text):\n",
    "        # Create a word index mapping each word to a unique index, with [UNK] token\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())  # Find all words in the text\n",
    "        sorted_words = sorted(set(words))  # Sort and remove duplicates\n",
    "        sorted_words.append(\"[UNK]\")  # Add an UNK token at the end\n",
    "        return {word: idx for idx, word in enumerate(sorted_words)}  # Create a dictionary mapping words to indices\n",
    "\n",
    "    def text_to_tensor(self):\n",
    "        # Convert the text into a tensor representation, with [UNK] handling\n",
    "        words = re.findall(r'\\b\\w+\\b', self.text.lower())  # Find all words in the text\n",
    "        int_sequence = [self.word_index.get(word, self.word_index[\"[UNK]\"]) for word in words]  # Convert words to indices\n",
    "        return torch.tensor(int_sequence, dtype=torch.long)  # Return as a tensor\n",
    "\n",
    "    def text_to_tensor_for_prompt(self, prompt):\n",
    "        # Convert the prompt into a tensor representation (based on how the words appear in self.dataset)\n",
    "        words = re.findall(r'\\b\\w+\\b', prompt.lower())  # Find all words in the prompt\n",
    "        int_sequence = [self.word_index.get(word, self.word_index[\"[UNK]\"]) for word in words]  # Convert words to indices\n",
    "        return torch.tensor(int_sequence, dtype=torch.long)  # Return as a tensor\n",
    "\n",
    "    def tensor_to_text(self, tensor):\n",
    "        # Convert the tensor back to words using the index_to_word mapping\n",
    "        word_list = [self.index_to_word.get(idx.item(), \"[UNK]\") for idx in tensor]  # Convert indices to words\n",
    "        return \" \".join(word_list)  # Join words into a single string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mTrainer\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprint_interval\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainer Constructor...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mTrainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTrainer\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: Transformer,\n\u001b[1;32m      4\u001b[0m                  text: \u001b[38;5;28mstr\u001b[39m, optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m----> 5\u001b[0m                  device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m                  sample_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, max_samples: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                  print_interval: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer Constructor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model: Transformer,\n",
    "                 text: str, optimizer: torch.optim.Optimizer,\n",
    "                 device: torch.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 sample_size: int = 1, max_samples: Optional[int] = None,\n",
    "                 print_interval: int = 1,\n",
    "                 epochs: int = 1):\n",
    "        print(\"Trainer Constructor...\")\n",
    "        self.model = model\n",
    "        self.text = text\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.sample_size = sample_size\n",
    "        self.max_samples = max_samples\n",
    "        self.print_interval = print_interval\n",
    "        self.epochs = epochs\n",
    "        self.dataset = TextFinder(text)\n",
    "        self.tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "        self.model.to(device)  # Move model to the specified device\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        data_samples = self.data_tensor.unfold(0, self.sample_size, self.sample_size)  # Create data samples\n",
    "        for data_sample in data_samples:\n",
    "            print(\"Data sample: \", data_sample)\n",
    "        return DataLoader(data_samples, batch_size=1, shuffle=False)  # Using DataLoader to load batches\n",
    "\n",
    "    def train(self):\n",
    "        print(f\"Tokenizing:\")\n",
    "\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # Set pre-tokenizer to Whitespace\n",
    "        trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2, show_progress=True)  # Initialize BPE trainer\n",
    "        self.tokenizer.train_from_iterator(self.text.split(), trainer)  # Train tokenizer on the text\n",
    "\n",
    "        encoded = self.tokenizer.encode(self.text)  # Encode the text\n",
    "        \n",
    "        self.data_tensor = torch.tensor(encoded.ids)  # Convert encoded text to tensor\n",
    "        self.dataloader = self.create_dataloader()  # Create dataloader\n",
    "\n",
    "        print(f\"Training with device: {self.device}\")\n",
    "        training_records: List[dict] = []\n",
    "        self.model.train()  # Set model to training mode\n",
    "        loss_values = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "            for i, sample in tqdm(enumerate(self.dataloader), total=len(self.dataloader), desc=\"Training\"):\n",
    "                sample = sample.squeeze(0)  # Remove extra dimension from the sample\n",
    "\n",
    "                inputs = sample[:-1]  # Inputs are all tokens except the last one\n",
    "                targets = sample[1:]  # Targets are all tokens except the first one\n",
    "\n",
    "                # forward pass\n",
    "                probabilities = self.model(inputs)  # Get model probabilities\n",
    "                log_probabilities = torch.log(probabilities)  # Compute log probabilities\n",
    "\n",
    "                # Calculate loss using NLLLoss\n",
    "                loss = F.nll_loss(log_probabilities.view(-1, log_probabilities.size(-1)), targets.view(-1))\n",
    "                \n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()  # Zero the gradients\n",
    "                loss.backward()  # Backpropagate the loss\n",
    "                self.optimizer.step()  # Update the model parameters\n",
    "\n",
    "                # record progress\n",
    "                training_records.append({\n",
    "                    \"sample\": i,\n",
    "                    \"loss\": loss.item(),\n",
    "                })\n",
    "                loss_values.append(loss.item())  # Store loss value\n",
    "\n",
    "                if i % self.print_interval == 0:\n",
    "                    print(f\"Sample {i}, Loss: {loss.item()}\")\n",
    "\n",
    "                if self.max_samples is not None and i >= self.max_samples:\n",
    "                    break\n",
    "                \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(loss_values, label=\"Training Loss\")  # Plot training loss\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Over Time\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        return self.model, training_records\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 50, temperature: float = 1.0) -> str:\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        encoded = self.tokenizer.encode(prompt)  # Encode the prompt\n",
    "        input_tensor = torch.tensor(encoded.ids).unsqueeze(0)  # Convert encoded prompt to tensor and add batch dimension\n",
    "\n",
    "        generated_tokens = input_tensor.squeeze(0).tolist()  # Initialize generated tokens list\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            logits = self.model(input_tensor)  # Get model logits\n",
    "            logits = logits[:, -1, :] / temperature  # Scale logits by temperature\n",
    "            probabilities = F.softmax(logits, dim=-1)  # Compute probabilities\n",
    "            next_token = torch.multinomial(probabilities, 1).item()  # Sample next token\n",
    "\n",
    "            generated_tokens.append(next_token)  # Append next token to generated tokens\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=self.device)], dim=1)  # Update input tensor\n",
    "\n",
    "        generated_text = self.tokenizer.decode(generated_tokens)  # Decode generated tokens to text\n",
    "\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting training data from training_data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_from_folder(data_folder: Path) -> str:\n",
    "    \"\"\"\n",
    "    Reads all text files from the specified folder and concatenates them into a single text string.\n",
    "    \"\"\"\n",
    "    text_data = \"\"\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = data_folder / file_name\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text_data += file.read() + \"\\n\"  # Adding newline after each file's content\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize GPT configuration\n",
    "    gpt_config = GPTConfig()\n",
    "    \n",
    "    # Initialize the Transformer model with the configuration\n",
    "    gpt_model = Transformer(gpt_config)\n",
    "\n",
    "    # Set up the optimizer with the model parameters and learning rate\n",
    "    optimizer = optim.Adam(gpt_model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Define the path to the training data folder\n",
    "    training_data_folder = Path(\"./training_data\")\n",
    "    \n",
    "    # Get the training data from the specified folder\n",
    "    some_book = get_training_data_from_folder(training_data_folder)\n",
    "    \n",
    "    # Initialize the Trainer with the model, training data, optimizer, and training parameters\n",
    "    trainer = Trainer(gpt_model, some_book, optimizer, epochs=1, sample_size=50, print_interval=100)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Train the model and get the trained model and training records\n",
    "    trained_model, training_records = trainer.train()\n",
    "\n",
    "    # Output the training records (losses)\n",
    "    print(\"Training complete.\")\n",
    "    for record in training_records:\n",
    "        print(f\"Sample {record['sample']}, Loss: {record['loss']}\")\n",
    "\n",
    "    print(\"**\" * 50)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    torch.save(trained_model, \"model.pt\")\n",
    "\n",
    "    # Generate text with the trained model using a prompt\n",
    "    prompt = \"Today I plan to complete the following tasks, \"\n",
    "    generated_text = trainer.generate(prompt)\n",
    "\n",
    "    print(\"Generated text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
