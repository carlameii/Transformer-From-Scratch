{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463488,)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "\t# default test values -- too small for a real language model, but big enough for testing\n",
    "\td_vocab: int = 10_000\n",
    "\td_model: int = 128\n",
    "\td_mlp: int = 512\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 32\n",
    "\tn_layers: int = 6\n",
    "\tact_fn: type[nn.Module] = nn.ReLU\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\t\"an estimate of the number of parameters\"\n",
    "\t\treturn (\n",
    "\t\t\tself.d_vocab * self.d_model # embeddings (and tied unembeddings)\n",
    "\t\t\t+ (\n",
    "\t\t\t\tself.d_model * self.d_mlp * 2 # mlp weights\n",
    "\t\t\t\t+ self.d_model + self.d_mlp # mlp bias\n",
    "\t\t\t\t+ self.n_heads * ( # number of heads\n",
    "\t\t\t\t\t4 * self.d_model * self.d_head # 4 because Q, K, O, V\n",
    "\t\t\t\t)\n",
    "\t\t\t) * self.n_layers, # for each layer\n",
    "\t\t)\n",
    "\t\n",
    "print(GPTConfig().n_params)\n",
    "\n",
    "# note: the residual stream is `n_context` by `d_model`\n",
    "\n",
    "# this is the row-wise (last dimension) softmax of x\n",
    "# F.softmax(x, dim=-1)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.d_vocab = cfg.d_vocab\n",
    "\t\tself.d_model = cfg.d_model\n",
    "\t\tself.d_head = cfg.d_head\n",
    "\t\tself.wq = nn.Linear(self.d_model, self.d_head)\n",
    "\t\tself.wk = nn.Linear(self.d_model, self.d_head)\n",
    "\t\tself.wv = nn.Linear(self.d_model, self.d_head)\n",
    "\t\tself.wo = nn.Linear(self.d_head, self.d_model)\n",
    "\n",
    "\t\n",
    "\t\n",
    "\tdef forward(self, x: Int[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tdef masking_matrix(n_context):\n",
    "\t\t\tmask = torch.zeros((n_context, n_context))  # Start with all 0s\n",
    "\t\t\tmask[torch.triu(torch.ones((n_context, n_context)), diagonal=1) == 1] = -float('inf')  # Set above diagonal to -inf\n",
    "\t\t\treturn mask\n",
    "\t\t\n",
    "\t\tM = masking_matrix(x.shape[0])\n",
    "\t\t# softmax_argument = x*self.wq*torch.transpose(self.wk)*torch.transpose(x) + M\n",
    "\t\twk_out = torch.transpose(self.wk(x), 0, 1)\n",
    "\t\t# print(\"WK shape \", wk_out.shape)\n",
    "\t\twq_out = self.wq(x)\n",
    "\t\t# print(\"WQ shape \", wq_out.shape)\n",
    "\t\tsoftmax_out = F.softmax((wq_out@wk_out + M), dim=-1)\n",
    "\t\t# print(\"Softmax shape \", softmax_out.shape)\n",
    "\t\twv_out = self.wv(x)\n",
    "\t\t# print(\"WV shape \", wv_out.shape)\n",
    "\t\two_out = self.wo(wv_out)\n",
    "\t\t# wo_out = self.wo(wv_out)\n",
    "\t\tresult = softmax_out@wo_out\n",
    "\t\t# print(\"Final A Shape \", result.shape)\n",
    "\t\treturn result\n",
    "\t\t\n",
    "\t\t\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = cfg.n_heads\n",
    "        self.d_model = cfg.d_model\n",
    "        self.d_head = cfg.d_head\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(cfg) for _ in range(self.n_heads)])\n",
    "\n",
    "        # Linear projection to project summed outputs back to d_model\n",
    "        self.wo = nn.Linear(self.d_model, self.d_model)  # Fix the output size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        head_outputs = [head(x) for head in self.attention_heads]\n",
    "        #print(\"Head output shape: \", head_outputs[0].shape)\n",
    "\n",
    "        summed_heads = torch.sum(torch.stack(head_outputs), dim=0)  # Sum over heads -> (n_context, d_head)\n",
    "\n",
    "        summed_heads += x  # Element-wise addition (ensures same shape)\n",
    "\n",
    "        # Project back to d_model\n",
    "        output = self.wo(summed_heads)  # (n_context, d_model)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# config\n",
    "\t\tself.d_model = cfg.d_model\n",
    "\t\tself.d_mlp = cfg.d_mlp\n",
    "\n",
    "\t\t# affine transformations\n",
    "\t\tself.lin1 = nn.Linear(self.d_model, self.d_mlp)\n",
    "\t\t# with nonlinearities in between\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.lin2 = nn.Linear(self.d_mlp, self.d_model)\n",
    "\n",
    "\tdef forward(self, x: Int[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\t# apply things in sequence\n",
    "\t\tout = self.lin1(x.flatten(start_dim=1))\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.lin2(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# uses `MultiHeadedAttention` and `MLP`\n",
    "\t\tself.multiheadattn = MultiHeadedAttention(cfg)\n",
    "\t\tself.mlp = MLP(cfg)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_vocab\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "\t\tout = self.multiheadattn(x)\n",
    "\t\tout = self.mlp(out) + x\n",
    "\t\treturn out\n",
    "\t\t\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\t\tself.unembedding = nn.Linear(cfg.d_model, cfg.d_vocab)\n",
    "\t\tself.transformer_blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "\t\t# uses `MultiHeadedAttention` and `MLP`\n",
    "\t\t# uses nn.Embedding for the embedding, transpose of it for the unembedding\n",
    "\n",
    "\tdef forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "\t\tout = self.embedding(x)\n",
    "\t\t#print(out.shape)\n",
    "\t\tfor block in self.transformer_blocks:\n",
    "\t\t\tout = block(out)\n",
    "\t\tout = F.softmax(self.unembedding(out), dim=-1)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "torch.Size([12, 10000])\n"
     ]
    }
   ],
   "source": [
    "gpt_config = GPTConfig()\n",
    "gpt = Transformer(gpt_config)\n",
    "x = torch.randint(0, gpt_config.d_vocab, (12,))\n",
    "print(x.shape)\n",
    "print(gpt(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Testing\n",
    "# gpt_config = GPTConfig()\n",
    "# attn_head = AttentionHead(gpt_config)\n",
    "# x = torch.randn(256, gpt_config.d_model)\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# attn_head.forward(x)\n",
    "# multi_head = MultiHeadedAttention(gpt_config)\n",
    "# multi_head.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([151, 275, 227,  ..., 125, 274, 173])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "some_text = \"\"\"\n",
    "In reality, of course, we don't construct such chains explicitly, but instead we want them to learn from data.\n",
    "\n",
    "To put something in a markov chain or neural network, we need to turn it into numbers. this is straightforward for images: each pixel is already a number! \n",
    "\n",
    "In computers, text is stored as a sequence of numbers. Our neural network, in principle, can learn to predict the next number in the sequence. However, each number usually represents a single letter, or even just part of a letter. what do you think happens when we throw something like this into a markov chain?\n",
    "ChatGPT said:\n",
    "When we throw text data into a Markov chain, especially when it's broken down into units like single letters or even parts of letters, the chain starts to model the statistical transitions between these units. Let's break this down:\n",
    "\n",
    "A Markov chain, by its nature, works by predicting the next state based on the current state, without any memory of the past beyond that. So, if we consider each letter or part of a letter as a state in the chain, the Markov model will simply calculate the probabilities of transitioning from one letter or part of a letter to the next.\n",
    "\n",
    "If we use individual letters as states, what happens is that the Markov chain doesn't get the full linguistic context. It may understand that certain letters follow other letters more frequently (e.g., \"e\" often follows \"t\" in English), but it doesn't understand words or meaning. The model, at best, can approximate patterns based on letter frequencies. This leads to the production of text that might look like it's composed of English letters but doesn't follow grammatical or semantic rules.\n",
    "\n",
    "Let's look at a basic example: If we train a Markov chain on a corpus of text and use individual letters as the states, the chain might learn something like:\n",
    "\n",
    "“t” → “h” → “e” → “r” → “e” → “s” → “h”\n",
    "This might produce something that looks a bit like the word “there,” but it might also just generate a sequence of random characters that have a similar statistical distribution, with no real coherence or meaning.\n",
    "\n",
    "Introducing a Larger Context\n",
    "To avoid generating gibberish and bring more coherence to the generated text, we need to move beyond individual characters or even parts of characters. In more advanced models, the chain would work with larger units of information: words or even entire phrases, where the chain can look at a broader context. This is essentially what neural networks, and more specifically, Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), do. They capture dependencies over larger sequences of text, considering not just the current word or character but the entire sequence that preceded it.\n",
    "\n",
    "Neural Networks and Word Embeddings\n",
    "Neural networks, especially those built for natural language processing, handle text much differently than a simple Markov chain. In a neural network, text is typically converted into numerical representations that capture semantic meaning, such as word embeddings. Each word (or even sub-word units) is mapped to a high-dimensional vector that represents its meaning relative to other words.\n",
    "\n",
    "When these word embeddings are fed into a neural network, the network doesn't just learn the sequence of letters or words in isolation. It learns relationships between words and their contexts. For instance, in a neural network trained on language, the words \"king\" and \"queen\" might have similar embeddings because they share semantic relationships, such as being royalty.\n",
    "\n",
    "Unlike a Markov chain, which might learn that certain letters follow each other based purely on statistical frequency, neural networks can learn the underlying patterns in meaning, sentence structure, and grammar. This means that they can generate more coherent text, because they understand not just which letters or words are likely to follow each other, but why they do.\n",
    "\n",
    "Transitioning from Markov Chains to Neural Networks\n",
    "Now, if we take the principles of a Markov chain and combine them with the power of neural networks, we get more advanced text generation models like GPT (Generative Pretrained Transformers), which are capable of generating fluent and contextually rich text. A Markov chain by itself has limitations, because it doesn't understand long-range dependencies well, whereas a neural network, particularly the transformer architecture, can model very long sequences of text by paying attention to different parts of the input at once, rather than just relying on the immediately preceding state.\n",
    "\n",
    "The Problem of Predicting the Next Number\n",
    "With that said, the problem of turning text into numbers isn't trivial. In a Markov chain, a system might simply predict the next unit (letter, part of a letter, or word) based on the previous one. But for a neural network, the model must map the input into vectors (numbers) that it can process. In order to do this, we typically use tokenization.\n",
    "\n",
    "For word-based models, each word in the vocabulary is assigned a unique number, and when we input text into the model, each word is converted into its corresponding number. The neural network, then, learns to predict the next word in the sequence by considering the numbers of previous words. This is done by training the model on large datasets, where it learns to predict the next token based on the preceding tokens in the text.\n",
    "\n",
    "However, if the text is represented in such a way that each character or part of a character is turned into a separate number, the network is then tasked with learning how to sequence these numbers, and while it can potentially learn patterns and regularities, it's much harder for the model to understand larger contextual dependencies. This is because the model lacks the broader semantic understanding that comes with grouping these parts into larger units (like words or phrases).\n",
    "\n",
    "A Simple Example\n",
    "Imagine a basic example where we input \"I love programming\" into a Markov chain or a neural network model that uses characters as the basic units. The network might break it down into:\n",
    "\n",
    "\"I\" → \" \" → \"l\" → \"o\" → \"v\" → \"e\" → \" \" → \"p\" → \"r\" → \"o\" → \"g\" → \"r\" → \"a\" → \"m\" → \"m\" → \"i\" → \"n\" → \"g\"\n",
    "If we trained a Markov chain to predict the next character, it might learn that after \"I\", a space often follows. After \"l\", \"o\" is likely to come next, and so on. While the chain could generate valid sequences of letters, they wouldn't necessarily form recognizable words or sentences. For example, it might output something like:\n",
    "\n",
    "\"I love p r o g\"\n",
    "\n",
    "which, while grammatically incomplete, still has some structure. However, it can't understand the semantics of \"programming\" as a whole concept.\n",
    "\n",
    "Now, if we use a neural network that works on whole words or sub-word units (like Byte Pair Encoding, or BPE, which splits words into frequent sub-word units), the model can better capture the semantics and coherence of the sentence. In this case, the network would learn that \"I\" and \"love\" are words that often go together, and it would also know the word \"programming\" as a single entity rather than trying to predict each character one by one.\n",
    "\n",
    "Deep Learning for Text Generation\n",
    "In deep learning models, such as transformers or GPT-like models, we don't just predict the next word based on the immediate previous word. Instead, these models use self-attention mechanisms to look at all the words in a sentence at once. This allows them to generate more coherent and contextually appropriate sequences. For example, when generating text, the model might consider the context of the entire sentence, rather than simply predicting the next word based on the previous one. This kind of context-aware generation leads to much more fluent, natural text.\n",
    "\n",
    "So, when we throw text into a Markov chain, we're dealing with a system that doesn't understand the relationships between words or even characters in any deep sense. It just knows the frequency of transitions. But a neural network model, by virtue of its architecture and learning approach, is able to model more complex dependencies and structures in the text, making it far more effective at generating meaningful, contextually appropriate language.\n",
    "\n",
    "Conclusion\n",
    "In conclusion, when we throw text into a Markov chain, especially one that deals with individual letters or parts of letters, we're left with a statistical model that generates sequences based on probabilities, but lacks the capacity to understand or produce meaningful language. The Markov chain would likely create some sequences that resemble text in shape, but they wouldn't be coherent or meaningful in the same way that a neural network trained on language can generate.\n",
    "\n",
    "Neural networks, particularly when using word embeddings or advanced models like transformers, are far more capable of capturing the complex relationships in language, allowing them to generate text that is not only syntactically correct but semantically rich as well. So, while a Markov chain can be a useful model for learning simple transitions, neural networks have the edge when it comes to understanding and generating real language.\n",
    "\"\"\"\n",
    "\n",
    "def create_word_index(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    sorted_words = sorted(set(words))\n",
    "    word_to_index = {word: idx for idx, word in enumerate(sorted_words)}\n",
    "\n",
    "    return word_to_index\n",
    "\n",
    "def text_to_tensor(vocab_dict, text):\n",
    "    # Remove punctuation and tokenize words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    # Convert words to their corresponding integer indices\n",
    "    int_sequence = [vocab_dict[word] for word in words if word in vocab_dict]\n",
    "\n",
    "    # Convert list to a PyTorch tensor\n",
    "    return torch.tensor(int_sequence, dtype=torch.long)\n",
    "\n",
    "print(text_to_tensor(create_word_index(some_text), some_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module, \n",
    "        optimizer: optim.Optimizer, \n",
    "        data_list: torch.tensor \n",
    "    ) -> None:\n",
    "    for index, (data) in enumerate(data_list):\n",
    "        # reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        # input: everything but the last element of the vectorized data\n",
    "        input = data[:-1]\n",
    "        # target: everything but the first element of the vectorized data\n",
    "        target = data[1:]\n",
    "\n",
    "        output = model(input)\n",
    "        # calculate the loss\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        # backward pass, update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(\n",
    "    model: nn.Module,\n",
    "    lr: float = 0.001\n",
    ") -> None:\n",
    "    # vectorize text\n",
    "    vectorized_text = text_to_tensor(create_word_index(some_text), some_text)\n",
    "    data_list = torch.split(vectorized_text, 5)\n",
    "\n",
    "    # set up the optimizer, based on the parameters of the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # train the model\n",
    "    train(\n",
    "        model = model,\n",
    "        optimizer = optimizer,\n",
    "        data_list = data_list)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gptconfig = GPTConfig()\n",
    "\n",
    "trained_model = main_train(\n",
    "    model = Transformer(gptconfig),\n",
    "    lr = 0.001\n",
    ")\n",
    "\n",
    "input_data = text_to_tensor(create_word_index(\"we don't construct such chains\"), some_text)\n",
    "output = trained_model(input_data)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
