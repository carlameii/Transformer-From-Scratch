{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer from Scratch\n",
    "\n",
    "### Authors:\n",
    " - Carla Ellefsen\n",
    " - Brendan McKinley\n",
    " - Diya Vinod\n",
    " - Bingshen Lu\n",
    " - Michael Ivanitskiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463488,)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # default test values -- too small for a real language model, but big enough for testing\n",
    "    d_vocab: int = 10_000\n",
    "    d_model: int = 128\n",
    "    d_mlp: int = 512\n",
    "    n_heads: int = 4\n",
    "    d_head: int = 32\n",
    "    n_layers: int = 6\n",
    "    act_fn: type[nn.Module] = nn.ReLU\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> tuple[int]:\n",
    "        \"an estimate of the number of parameters\"\n",
    "        return (\n",
    "            self.d_vocab * self.d_model  # embeddings (and tied unembeddings)\n",
    "            + (\n",
    "                    self.d_model * self.d_mlp * 2  # mlp weights\n",
    "                    + self.d_model + self.d_mlp  # mlp bias\n",
    "                    + self.n_heads * (  # number of heads\n",
    "                            4 * self.d_model * self.d_head  # 4 because Q, K, O, V\n",
    "                    )\n",
    "            ) * self.n_layers,  # for each layer\n",
    "        )\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        print(\"Attention Head Constructor...\")\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.d_vocab = cfg.d_vocab\n",
    "        self.d_model = cfg.d_model\n",
    "        self.d_head = cfg.d_head\n",
    "        self.wq = nn.Linear(self.d_model, self.d_head)\n",
    "        self.wk = nn.Linear(self.d_model, self.d_head)\n",
    "        self.wv = nn.Linear(self.d_model, self.d_head)\n",
    "        self.wo = nn.Linear(self.d_head, self.d_model)\n",
    "\n",
    "    def forward(self,\n",
    "                x: Int[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        def masking_matrix(n_context):\n",
    "            mask = torch.zeros((n_context, n_context))  # Start with all 0s\n",
    "            mask[torch.triu(torch.ones((n_context, n_context)), diagonal=1) == 1] = -float('inf')  # Set above diagonal to -inf\n",
    "            return mask\n",
    "\n",
    "        M = masking_matrix(x.shape[0])\n",
    "        # softmax_argument = x*self.wq*torch.transpose(self.wk)*torch.transpose(x) + M\n",
    "        # wk_out = torch.transpose(self.wk(x), 0, 1)\n",
    "        wk_out = self.wk(x).transpose(-2, -1)  # Correct transposition\n",
    "\n",
    "        # wk_out = self.wk(x).transpose(-2, -1)\n",
    "        # print(\"WK shape \", wk_out.shape)\n",
    "        # print(\"WK shape: torch.transpose(self.wk(x), 0, 1) \", torch.transpose(self.wk(x), 0, 1).shape)\n",
    "        # print(\"WK shape: self.wk(x).transpose(-2, -1) \", self.wk(x).transpose(-2, -1).shape)\n",
    "\n",
    "        wq_out = self.wq(x)\n",
    "        # print(\"WQ shape \", wq_out.shape)\n",
    "        softmax_out = F.softmax((wq_out @ wk_out + M), dim=-1)\n",
    "\n",
    "        # print(\"Softmax shape \", softmax_out.shape)\n",
    "        wv_out = self.wv(x)\n",
    "        # print(\"WV shape \", wv_out.shape)\n",
    "        wo_out = self.wo(wv_out)\n",
    "\n",
    "        result = softmax_out @ wo_out\n",
    "        # print(\"Final A Shape \", result.shape)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing\n",
    "Ensure the code does not crash and shapes are as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "Head output shape:  torch.Size([12, 128])\n",
      "torch.Size([12, 10000])\n"
     ]
    }
   ],
   "source": [
    "gpt_config = GPTConfig()\n",
    "gpt = Transformer(gpt_config)\n",
    "x = torch.randint(0, gpt_config.d_vocab, (12,))\n",
    "print(x.shape)\n",
    "print(gpt(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Testing\n",
    "# gpt_config = GPTConfig()\n",
    "# attn_head = AttentionHead(gpt_config)\n",
    "# x = torch.randn(256, gpt_config.d_model)\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# attn_head.forward(x)\n",
    "# multi_head = MultiHeadedAttention(gpt_config)\n",
    "# multi_head.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 45, 37,  9, 64, 12, 53,  8, 52,  6, 15,  3, 22, 64, 63, 56, 60, 27,\n",
      "        17, 10, 60, 44, 49, 21,  0, 30,  5, 38, 33, 32, 64, 31, 60, 61, 25, 23,\n",
      "        36, 58, 24, 51, 16, 20, 13, 41, 24,  1,  0, 35, 21,  7, 54, 24, 50,  2,\n",
      "         0, 47, 37, 36, 39, 33, 32, 21, 43,  4, 27, 60, 42, 55, 34, 35, 21, 55,\n",
      "        47, 19, 13, 35, 62, 46,  0, 48, 28, 38, 14, 26, 40, 37,  0, 28, 65, 11,\n",
      "        67, 57, 18, 66, 64, 59, 49, 29, 58, 23,  0, 30,  5])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "some_text = \"\"\"\n",
    "In reality, of course, we don't construct such chains explicitly, but instead we want them to learn from data.\n",
    "\n",
    "To put something in a markov chain or neural network, we need to turn it into numbers. this is straightforward for images: each pixel is already a number! \n",
    "\n",
    "In computers, text is stored as a sequence of numbers. Our neural network, in principle, can learn to predict the next number in the sequence. However, each number usually represents a single letter, or even just part of a letter. what do you think happens when we throw something like this into a markov chain?\n",
    "\"\"\"\n",
    "\n",
    "def create_word_index(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    sorted_words = sorted(set(words))\n",
    "    word_to_index = {word: idx for idx, word in enumerate(sorted_words)}\n",
    "\n",
    "    return word_to_index\n",
    "\n",
    "def text_to_tensor(vocab_dict, text):\n",
    "    # Remove punctuation and tokenize words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    # Convert words to their corresponding integer indices\n",
    "    int_sequence = [vocab_dict[word] for word in words if word in vocab_dict]\n",
    "\n",
    "    # Convert list to a PyTorch tensor\n",
    "    return torch.tensor(int_sequence, dtype=torch.long)\n",
    "\n",
    "print(text_to_tensor(create_word_index(some_text), some_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
